from langchain_community.vectorstores import FAISS
from langchain.schema import Document
import os

class DatabaseManager:
    def __init__(self, db_path, embeddings, initial_documents=None):
        self.db_path = db_path
        self.embeddings = embeddings
        self.db = self._load_or_create_db(initial_documents or [])  # åˆå§‹è³‡æ–™é›†ä¸å¯ç‚ºç©º

    def _load_or_create_db(self, documents):
        if os.path.exists(self.db_path):
            db = FAISS.load_local(self.db_path, self.embeddings, allow_dangerous_deserialization=True)
            print("âœ… å·²åŠ è¼‰ç¾æœ‰è³‡æ–™åº«")
        else:
            if not documents:
                raise ValueError("âŒ ç„¡æ³•å»ºç«‹è³‡æ–™åº«ï¼šåˆå§‹è³‡æ–™ç‚ºç©º")
            db = FAISS.from_documents(documents, self.embeddings)
            db.save_local(self.db_path)
            print("âœ… å·²å»ºç«‹ä¸¦å„²å­˜è³‡æ–™åº«")
        return db

    def search_data(self, query, k=2):
        result = self.db.similarity_search(query, k=k)
        print("\nğŸ” æŸ¥è©¢çµæœï¼š")
        for i, doc in enumerate(result):
            print(f"{i + 1}. {doc.page_content}")

    def add_data(self, new_texts):
        new_docs = [Document(page_content=text) for text in new_texts]
        self.db.add_documents(new_docs)
        self.db.save_local(self.db_path)
        print("âœ… è³‡æ–™å·²æˆåŠŸæ–°å¢")

    def delete_data(self, delete_text):
        all_texts = [doc.page_content for doc in self.db.similarity_search("", k=100)]
        filtered_texts = [text for text in all_texts if text != delete_text]

        self.db = FAISS.from_documents([Document(page_content=text) for text in filtered_texts], self.embeddings)
        self.db.save_local(self.db_path)
        print(f"âœ… å·²æˆåŠŸåˆªé™¤è³‡æ–™: {delete_text}")

    def update_data(self, old_text, new_text):
        self.delete_data(old_text)
        self.add_data([new_text])
        print(f"âœ… å·²å°‡è³‡æ–™ '{old_text}' æ›´æ–°ç‚º '{new_text}'")