from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.embeddings.base import Embeddings
import torch.nn.functional as F
import os
import torch
from torch import Tensor

class DatabaseManager:
    def __init__(self, db_path, embeddings, initial_documents=None):
        self.db_path = db_path
        self.embeddings = embeddings
        self.db = self._load_or_create_db(initial_documents or [])

    def _load_or_create_db(self, documents):
        if os.path.exists(self.db_path):
            db = FAISS.load_local(self.db_path, self.embeddings, allow_dangerous_deserialization=True)
            print("âœ… å·²åŠ è¼‰ç¾æœ‰è³‡æ–™åº«")
        else:
            if not documents:
                raise ValueError("âŒ ç„¡æ³•å»ºç«‹è³‡æ–™åº«ï¼šåˆå§‹è³‡æ–™ç‚ºç©º")
            db = FAISS.from_documents(documents, self.embeddings)
            db.save_local(self.db_path)
            print("âœ… å·²å»ºç«‹ä¸¦å„²å­˜è³‡æ–™åº«")
        return db

    def search_data(self, query, k=2):
        result = self.db.similarity_search(query, k=k)
        print("\nğŸ” æŸ¥è©¢çµæœï¼š")
        resultText = ""
        for i, doc in enumerate(result):
            print(f"{i + 1}. {doc.page_content}")
            resultText += f'[{doc.page_content}],' 
        return resultText

    def add_data(self, new_texts):
        new_docs = [Document(page_content=text.strip()) for text in new_texts]
        self.db.add_documents(new_docs)
        self.db.save_local(self.db_path)
        print("âœ… è³‡æ–™å·²æˆåŠŸæ–°å¢")

    def delete_data(self, delete_text):
        all_texts = [doc.page_content for doc in self.db.similarity_search("", k=100)]
        filtered_texts = [text for text in all_texts if text != delete_text]

        self.db = FAISS.from_documents([Document(page_content=text) for text in filtered_texts], self.embeddings)
        self.db.save_local(self.db_path)
        print(f"âœ… å·²æˆåŠŸåˆªé™¤è³‡æ–™: {delete_text}")

    def update_data(self, old_text, new_text):
        self.delete_data(old_text)
        self.add_data([new_text])
        print(f"âœ… å·²å°‡è³‡æ–™ '{old_text}' æ›´æ–°ç‚º '{new_text}'")

# Average pooling function
def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

# è‡ªè¨‚ Embedding é¡åˆ¥ class
class CustomEmbedding(Embeddings):
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def embed_documents(self, texts):
        embeddings_list = []
        batch_size = 8  # æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            batch_dict = self.tokenizer(batch_texts, max_length=512, padding=True, truncation=True, return_tensors='pt').to(self.device)
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦ä»¥é™ä½è¨˜æ†¶é«”æ¶ˆè€—
                outputs = self.model(**batch_dict)

            embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
            embeddings = F.normalize(embeddings, p=2, dim=1).cpu()
            embeddings_list.append(embeddings)
            print(f'{i}/{len(texts)}')

        return torch.cat(embeddings_list, dim=0).tolist()  # FAISS éœ€è¦ List æ ¼å¼

    def embed_query(self, text):
        return self.embed_documents([text])[0]
