from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
import os

# åˆå§‹åŒ– Ollama Embeddings (ä½¿ç”¨ llama3 æ¨¡å‹)
embeddings = OllamaEmbeddings(model="llama3")

# æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å·²å­˜åœ¨
DB_PATH = "Database"

# ğŸ”¹ åˆå§‹è³‡æ–™
documents = [
    Document(page_content="å°ç£æ˜¯äºæ´²ä¸€å€‹ç¾éº—çš„å³¶å¶¼ï¼Œä»¥ç¾é£Ÿå’Œå‹å–„çš„äººæ°‘èåã€‚"),
    Document(page_content="æ—¥æœ¬ä»¥å…¶å‚³çµ±æ–‡åŒ–å’Œç¾ä»£ç§‘æŠ€ä¸¦å­˜çš„ç¨ç‰¹é¢¨è²Œèåæ–¼ä¸–ã€‚"),
    Document(page_content="ç¾åœ‹æ“æœ‰å¤šæ¨£åŒ–çš„æ–‡åŒ–å’Œé¢¨æ™¯ï¼Œä¸¦ä»¥ç§‘æŠ€ç™¼å±•å’Œå‰µæ–°è‘—ç¨±ã€‚"),
    Document(page_content="æ³•åœ‹ä»¥å…¶ç¾é£Ÿã€æ™‚å°šå’Œè—è¡“èåï¼Œæ˜¯æ­æ´²æ—…éŠç†±é–€ç›®çš„åœ°ã€‚"),
]

# ğŸ”¹ å»ºç«‹æˆ–è¼‰å…¥è³‡æ–™åº«
def load_or_create_db(db_path, documents):
    if os.path.exists(db_path):
        db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        print("âœ… å·²åŠ è¼‰ç¾æœ‰è³‡æ–™åº«")
    else:
        db = FAISS.from_documents(documents, embeddings)
        db.save_local(db_path)
        print("âœ… å·²å»ºç«‹ä¸¦å„²å­˜è³‡æ–™åº«")
    return db

# ğŸ”¹ æŸ¥è©¢è³‡æ–™ (Search)
def search_data(db, query, k=2):
    result = db.similarity_search(query, k=k)
    print("\nğŸ” æŸ¥è©¢çµæœï¼š")
    for i, doc in enumerate(result):
        print(f"{i + 1}. {doc.page_content}")

# ğŸ”¹ æ–°å¢è³‡æ–™ (Add)
def add_data(db, new_texts, db_path):
    new_docs = [Document(page_content=text) for text in new_texts]
    db.add_documents(new_docs)
    db.save_local(db_path)
    print("âœ… è³‡æ–™å·²æˆåŠŸæ–°å¢")

# ğŸ”¹ åˆªé™¤è³‡æ–™ (Delete)
def delete_data(db, delete_text, db_path):
    all_texts = [doc.page_content for doc in db.similarity_search("", k=100)]
    filtered_texts = [text for text in all_texts if text != delete_text]

    db = FAISS.from_documents([Document(page_content=text) for text in filtered_texts], embeddings)
    db.save_local(db_path)
    print(f"âœ… å·²æˆåŠŸåˆªé™¤è³‡æ–™: {delete_text}")
    return db  # æ›´æ–°å¾Œçš„ db éœ€è¿”å›ä»¥ä¾›å¾ŒçºŒä½¿ç”¨

# ğŸ”¹ æ›´æ–°è³‡æ–™ (Update)
def update_data(db, old_text, new_text, db_path):
    db = delete_data(db, old_text, db_path)  # å…ˆåˆªé™¤èˆŠè³‡æ–™
    add_data(db, [new_text], db_path)        # å†æ–°å¢æ–°è³‡æ–™
    print(f"âœ… å·²å°‡è³‡æ–™ '{old_text}' æ›´æ–°ç‚º '{new_text}'")

# === æ¸¬è©¦å€ ===
db = load_or_create_db(DB_PATH, documents)

# æ¸¬è©¦æŸ¥è©¢
search_data(db, "äºæ´²çš„ç¾éº—å³¶å¶¼")  

# æ¸¬è©¦æ–°å¢
add_data(db, ["å¾·åœ‹ä»¥å…¶ç²¾å¯†å·¥ç¨‹å’Œæ±½è»Šå·¥æ¥­èåã€‚"], DB_PATH)  
search_data(db, "æ±½è»Šå·¥æ¥­")  


# # æ¸¬è©¦æ›´æ–°
# update_data(db, "å°ç£æ˜¯äºæ´²ä¸€å€‹ç¾éº—çš„å³¶å¶¼ï¼Œä»¥ç¾é£Ÿå’Œå‹å–„çš„äººæ°‘èåã€‚", "å°ç£æ“æœ‰è±å¯Œçš„å¤œå¸‚æ–‡åŒ–ï¼Œå¸å¼•è¨±å¤šè§€å…‰å®¢ã€‚", DB_PATH)  
# search_data(db, "å°ç£")  

# # æ¸¬è©¦åˆªé™¤
# db = delete_data(db, "æ—¥æœ¬ä»¥å…¶å‚³çµ±æ–‡åŒ–å’Œç¾ä»£ç§‘æŠ€ä¸¦å­˜çš„ç¨ç‰¹é¢¨è²Œèåæ–¼ä¸–ã€‚", DB_PATH)  
# search_data(db, "æ—¥æœ¬")  


# # ä½¿ç”¨ similarity_search_with_score() ä¾†å–å¾—ç›¸ä¼¼åº¦åˆ†æ•¸
# results = db.similarity_search_with_score(query, k=2)

# # é¡¯ç¤ºçµæœ (æŒ‰ç›¸ä¼¼åº¦æ’åº)
# print("ğŸ” æŸ¥è©¢çµæœ (åŒ…å«ç›¸ä¼¼åº¦åˆ†æ•¸)ï¼š")
# for i, (doc, score) in enumerate(sorted(results, key=lambda x: x[1], reverse=True), 1):
#     print(f"{i}. {doc.page_content} (ç›¸ä¼¼åº¦: {score:.4f})")