from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from rag_database import DatabaseManager, CustomEmbedding
from transformers import AutoTokenizer as E5Tokenizer, AutoModel as E5Model

# åˆå§‹åŒ–ç”Ÿæˆå¼æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-27b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-27b-it",
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# åˆå§‹åŒ– Embedding æ¨¡å‹
tok = E5Tokenizer.from_pretrained('intfloat/multilingual-e5-base')
mod = E5Model.from_pretrained('intfloat/multilingual-e5-base').half().to(device)
embedding_model = CustomEmbedding(mod, tok, device)

# åˆå§‹åŒ–è³‡æ–™åº«
DB_PATH = "Law"
db_manager = DatabaseManager(DB_PATH, embedding_model)

# è®€å–æ‰€æœ‰é—œéµå­—
with open("keyword.txt", "r", encoding="utf-8") as f:
    keywords = [line.strip() for line in f if line.strip()]

# å„²å­˜æ‰€æœ‰ QA çµæœ
all_qa_results = []

# éæ­·æ‰€æœ‰é—œéµå­—ä¸¦ç”Ÿæˆ QA
for i, keyword in enumerate(keywords, 1):
    print(f"ğŸ” [{i}/{len(keywords)}] è™•ç†é—œéµå­—: {keyword}")
    
    try:
        context = db_manager.search_data(keyword, 5)
        # input_text = (
        #     f"è«‹åƒè€ƒä»¥ä¸‹æ³•å¾‹æ¢æ–‡ä¾†ç”¢ç”Ÿ5å€‹ç‰¹å®šä¸»é¡Œçš„å¸¸è¦‹å•ç­”QAï¼Œ"
        #     f"æ¯å€‹QAé ˆåŒ…å«ä¸€å€‹å•å¥åŠä¸€å€‹ç­”å¥ï¼Œæ ¼å¼å¦‚ä¸‹"
        #     f"å•å¥:(è«‹å¡«å…¥å•å¥1)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥1)\nå•å¥:(è«‹å¡«å…¥å•å¥2)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥2)\nå•å¥:(è«‹å¡«å…¥å•å¥3)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥3)\nå•å¥:(è«‹å¡«å…¥å•å¥4)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥4)\nå•å¥:(è«‹å¡«å…¥å•å¥5)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥5)\n"
        #     f"ç‰¹å®šä¸»é¡Œ:{keyword}ï¼Œæ³•æ¢ç‰‡æ®µ:{context}"
        # )
        input_text = (
            f"è«‹åƒè€ƒä»¥ä¸‹æ³•å¾‹æ¢æ–‡ä¾†ç”¢ç”Ÿ5å€‹ç‰¹å®šä¸»é¡Œçš„å¸¸è¦‹å•ç­”QAï¼Œ"
            f"æ¯å€‹QAé ˆåŒ…å«ä¸€å€‹å•å¥åŠä¸€å€‹ç­”å¥ï¼Œæ ¼å¼å¦‚ä¸‹"
            f"é—œéµå­—:(è«‹å¡«å…¥é—œéµå­—1)\nå•å¥:(è«‹å¡«å…¥å•å¥1)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥1)\né—œéµå­—:(è«‹å¡«å…¥é—œéµå­—2)\nå•å¥:(è«‹å¡«å…¥å•å¥2)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥2)\né—œéµå­—:(è«‹å¡«å…¥é—œéµå­—3)\nå•å¥:(è«‹å¡«å…¥å•å¥3)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥3)\né—œéµå­—:(è«‹å¡«å…¥é—œéµå­—4)\nå•å¥:(è«‹å¡«å…¥å•å¥4)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥4)\né—œéµå­—:(è«‹å¡«å…¥é—œéµå­—5)\nå•å¥:(è«‹å¡«å…¥å•å¥5)\nç­”å¥:(è«‹å¡«å…¥ç­”å¥5)\n"
            f"ç‰¹å®šä¸»é¡Œ:{keyword}ï¼Œæ³•æ¢ç‰‡æ®µ:{context}"
        )

        input_ids = tokenizer(input_text, return_tensors="pt").to(device)
        outputs = model.generate(**input_ids, max_new_tokens=1024)
        generated_ids = outputs[0][input_ids['input_ids'].shape[1]:]
        output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        # å°‡çµæœå„²å­˜é€²è®Šæ•¸
        qa_block = f"ğŸ”‘ é—œéµå­—: {keyword}\n{output_text}\n{'='*60}\n"
        all_qa_results.append(qa_block)

        print(qa_block)

    except Exception as e:
        error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤æ–¼é—œéµå­—ã€Œ{keyword}ã€: {e}\n{'='*60}\n"
        all_qa_results.append(error_msg)
        print(error_msg)

# å°‡æ‰€æœ‰ QA å¯«å…¥æª”æ¡ˆ
with open("qa2.txt", "w", encoding="utf-8") as f:
    f.writelines(all_qa_results)

print("âœ… æ‰€æœ‰ QA çµæœå·²å¯«å…¥ qa.txt")
