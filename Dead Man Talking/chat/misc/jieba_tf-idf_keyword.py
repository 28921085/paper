import re
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer

# è®€å–æ•´ä»½ law.txt
with open("law.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# ä¿®æ­£ï¼šæ”¯æ´ xxxx æ¢ / xxxx-x æ¢
pattern = r"(ç¬¬\s*\d+(?:-\d+)?\s*æ¢)\s*\n?(.+?)(?=(ç¬¬\s*\d+(?:-\d+)?\s*æ¢|$))"
matches = re.findall(pattern, raw_text, re.DOTALL)

# law_paragraphs: å„²å­˜æ¯æ¢æ³•æ¢çš„å®Œæ•´å…§æ–‡
law_paragraphs = [f"{m[0]}\n{m[1].strip()}" for m in matches]

# jieba æ–·è©žï¼ˆå¯åŠ å…¥è‡ªè¨‚è©žå…¸ï¼‰
tokenized_docs = [" ".join(jieba.lcut(para)) for para in law_paragraphs]

# è¨ˆç®— TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(tokenized_docs)
feature_names = vectorizer.get_feature_names_out()

# é¡¯ç¤ºæ¯æ¢æ³•æ¢çš„ top K é—œéµè©ž
top_k = 10
with open("keyword_info.txt", "w", encoding="utf-8") as f:
    for i, row in enumerate(tfidf_matrix.toarray()):
        # æ³•æ¢æ¨™é¡Œï¼ˆå¦‚ï¼šç¬¬ 1080 æ¢ï¼‰
        title = law_paragraphs[i].splitlines()[0]
        f.write(f"\nðŸ“„ {title} é—œéµå­—ï¼š\n")
        
        # å–å¾— top K é—œéµè©žçš„ç´¢å¼•
        top_indices = row.argsort()[-top_k:][::-1]
        for idx in top_indices:
            score = row[idx]
            if score > 0:
                f.write(f"  {feature_names[idx]}: {score:.4f}\n")
