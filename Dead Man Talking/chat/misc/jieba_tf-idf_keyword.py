import jieba
from sklearn.feature_extraction.text import TfidfVectorizer

# Step 1: è®€å–å…¨æ–‡
with open("law.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# Step 2: ä½¿ç”¨ jieba æ–·è©ï¼ˆæ•´ä»½ä¸€æ¬¡è™•ç†ï¼‰
tokenized_text = " ".join(jieba.lcut(raw_text))

# Step 3: TF-IDFï¼ˆåªåˆ†æä¸€ä»½ã€Œæ•´é«”æ–‡ä»¶ã€ï¼‰
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([tokenized_text])
feature_names = vectorizer.get_feature_names_out()
tfidf_scores = tfidf_matrix.toarray()[0]  # åªæœ‰ä¸€ä»½æ–‡ä»¶

# Step 4: å– top k é—œéµå­—
top_k = 50
top_indices = tfidf_scores.argsort()[-top_k:][::-1]

# Step 5: å¯«å…¥æª”æ¡ˆ
with open("keyword_info.txt", "w", encoding="utf-8") as f:
    f.write("ğŸ“„ å…¨æ–‡å‰ Top {} é—œéµå­—ï¼š\n".format(top_k))
    for idx in top_indices:
        score = tfidf_scores[idx]
        f.write(f"  {feature_names[idx]}: {score:.4f}\n")
