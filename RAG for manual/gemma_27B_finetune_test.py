# auto_infer_from_file.py

from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModel
import torch
from rag_database import DatabaseManager, CustomEmbedding

# è¼‰å…¥ fine-tuned æ¨¡å‹
# model_dir = "lora_gemma_adapter" # fine tune
model_dir = "google/gemma-2-27b-it" #base
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)
model.eval()

#rag
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tok = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')
mod = AutoModel.from_pretrained('intfloat/multilingual-e5-base').half().to(device)
embedding_model = CustomEmbedding(mod, tok, device)
# DB_PATH = "Law"
DB_PATH = "Manual"

db_manager = DatabaseManager(DB_PATH, embedding_model)

# å•ç­”å‡½å¼
def ask_question(question: str) -> str:
    prompt = f"<start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n" # base

    # context = db_manager.search_data(question, 5,False)
    # print(context)
    # # prompt = f"è«‹é–±è®€ä¸Šä¸‹æ–‡å¾Œå›ç­”é€™å€‹å•é¡Œ\nå•é¡Œ:{question}\nä¸Šä¸‹æ–‡{context}\n"
    # prompt = f"Please read the context before answering this question.\nQuestion:{question}\nContext:{context}\n"
    # prompt = f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response=response.split("model\n")[-1]
    return response

# å¾æª”æ¡ˆä¸­æ“·å–å•é¡Œ
def extract_questions(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # return [line.strip()[2:].strip() for line in lines if line.strip().startswith("Q:")]
    return [line.strip()[9:].strip() for line in lines if line.strip().startswith("Question:")]


# ä¸»ç¨‹å¼ï¼šåŸ·è¡Œæ¨è«–èˆ‡ç´€éŒ„
questions = extract_questions("qa_ex_en.txt")
# questions = extract_questions("qa_ex.txt")


with open("qa_record.txt", "w", encoding="utf-8") as f:
    for i, question in enumerate(questions, 1):
        answer = ask_question(question)
        print(f"â¡ï¸ æ­£åœ¨è™•ç†ç¬¬ {i} é¡Œï¼š{question}")
        print(f"ğŸ¤– å›ç­”ï¼š{answer}\n")
        f.write(f"Q: {question}\nA: {answer}\n\n")

print("âœ… æ‰€æœ‰å•ç­”å·²å®Œæˆä¸¦å„²å­˜è‡³ qa_record.txt")
