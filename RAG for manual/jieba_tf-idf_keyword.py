import re
import jieba
import math
from sklearn.feature_extraction.text import TfidfVectorizer

# è®€å–æ•´ä»½ law.txt
with open("law.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# æ”¯æ´ xxxx æ¢ / xxxx-x æ¢
pattern = r"(ç¬¬\s*\d+(?:-\d+)?\s*æ¢)\s*\n?(.+?)(?=(ç¬¬\s*\d+(?:-\d+)?\s*æ¢|$))"
matches = re.findall(pattern, raw_text, re.DOTALL)

# law_paragraphs: å„²å­˜æ¯æ¢æ³•æ¢çš„å®Œæ•´å…§æ–‡
law_paragraphs = [f"{m[0]}\n{m[1].strip()}" for m in matches]
print(len(law_paragraphs))

# å°‡æ¯ group_size æ¢åˆä½µæˆä¸€å€‹æ®µè½
group_size = int(math.sqrt(len(law_paragraphs)))
grouped_paragraphs = [
    "\n".join(law_paragraphs[i:i+group_size])
    for i in range(0, len(law_paragraphs), group_size)
]

# jieba æ–·è©è™•ç†
tokenized_groups = [" ".join(jieba.lcut(group)) for group in grouped_paragraphs]

# è¨ˆç®— TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(tokenized_groups)
feature_names = vectorizer.get_feature_names_out()

# ğŸ” çµ±è¨ˆæ¯å€‹é—œéµå­—å‡ºç¾éçš„æœ€é«˜åˆ†æ•¸
keyword_max_score = {}

for row in tfidf_matrix.toarray():
    for idx, score in enumerate(row):
        if score > 0:
            word = feature_names[idx]
            if word not in keyword_max_score or score > keyword_max_score[word]:
                keyword_max_score[word] = score

# ğŸ”¢ æ’åºä¸¦å–å‰ 200 å
top_keywords = sorted(keyword_max_score.items(), key=lambda x: x[1], reverse=True)[:200]

with open("keyword.txt", "w", encoding="utf-8") as f:
    # f.write("ğŸ“Š å‰ 200 å€‹é«˜ TF-IDF é—œéµå­—ï¼ˆå–æœ€å¤§åˆ†æ•¸ï¼‰ï¼š\n")
    for word, score in top_keywords:
        # f.write(f"{word}: {score:.4f}\n")
        f.write(f"{word}\n")

