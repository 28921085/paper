from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from rag_database import DatabaseManager, CustomEmbedding
from transformers import AutoTokenizer as E5Tokenizer, AutoModel as E5Model
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# 1ï¸âƒ£ åˆå§‹åŒ–ç”Ÿæˆæ¨¡å‹ï¼ˆGemmaï¼‰
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-27b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-27b-it",
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 2ï¸âƒ£ åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆE5ï¼‰
tok = E5Tokenizer.from_pretrained('intfloat/multilingual-e5-base')
mod = E5Model.from_pretrained('intfloat/multilingual-e5-base').half().to(device)
embedding_model = CustomEmbedding(mod, tok, device)


# 4ï¸âƒ£ è¼‰å…¥ manual.txt ä¸¦åˆ‡å‰²æˆ chunks
with open("manual.txt", "r", encoding="utf-8") as f:
    content = f.read()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_text(content)

# 5ï¸âƒ£ å»ºç«‹ QA çµæœå®¹å™¨
all_qa_results = []

# 6ï¸âƒ£ éæ­·æ¯å€‹ chunkï¼Œç”¢ç”Ÿ QA
for i, chunk in enumerate(chunks, 1):
    print(f"ğŸ” [{i}/{len(chunks)}] è™•ç†å€å¡Š...")

    try:
        input_text = (
            f"You are currently reading the company's employee handbook. Please refer to the following text and generate 3 common Q&A pairs. "
            f"Each pair should include a **keyword**, a **question**, and an **answer**, formatted as follows:\n"
            f"Keyword: (Insert keyword 1)\nQuestion: (Insert question 1)\nAnswer: (Insert answer 1)\n"
            f"Keyword: (Insert keyword 2)\nQuestion: (Insert question 2)\nAnswer: (Insert answer 2)\n"
            f"Keyword: (Insert keyword 3)\nQuestion: (Insert question 3)\nAnswer: (Insert answer 3)\n\n"
            f"Below is the text content:\n{chunk}"
        )

        input_ids = tokenizer(input_text, return_tensors="pt").to(device)
        outputs = model.generate(**input_ids, max_new_tokens=1024)
        generated_ids = outputs[0][input_ids['input_ids'].shape[1]:]
        output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        qa_block = f"ğŸ§© Chunk {i}:\n{output_text}\n{'='*60}\n"
        all_qa_results.append(qa_block)
        print(qa_block)

    except Exception as e:
        error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤æ–¼ chunk {i}: {e}\n{'='*60}\n"
        all_qa_results.append(error_msg)
        print(error_msg)

# 7ï¸âƒ£ è¼¸å‡ºæ‰€æœ‰ QA åˆ°æª”æ¡ˆ
with open("qa_from_chunks.txt", "w", encoding="utf-8") as f:
    f.writelines(all_qa_results)

print("âœ… æ‰€æœ‰ QA çµæœå·²å¯«å…¥ qa_from_chunks.txt")
